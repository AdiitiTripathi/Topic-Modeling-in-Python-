{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "    For this project, I have used topic modeling to deliver marketing and product insights for SPEEDO.\n",
    "\n",
    "# The Data\n",
    "\n",
    "    Prof. Julian McAuley at UC-San Diego has graciously let me use his “Amazon Product Data. It contains tons of data about Amazon products. Specifically, I have leveraged two datasets: (1) meta-data about products & (2) product reviews. The aforementioned database has reviews on all types of Amazon products,but these datasets are huge (~80gb).\n",
    "\n",
    "    To remedy this, I’ve picked to two smaller datasets that only contain (1) meta-data about products that are in    categorized as “Clothing, Shoes & Jewelry” and (2) reviews about products that are in the “Clothing, Shoes &    Jewelry” category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps:\n",
    "    Firstly, I extracted the Asins(unique identifier) for all products of SPEEDO from the \n",
    "    Metadata of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "loadedjson = open('meta_Clothing_Shoes_and_Jewelry.json','r')\n",
    "\n",
    "# loading the json metadata of products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n"
     ]
    }
   ],
   "source": [
    "allproducts = {}    #making a dictionary\n",
    "listofcategories = {}\n",
    "\n",
    "for aline in loadedjson:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(count)\n",
    "    aproduct = eval(aline)\n",
    "    \n",
    "    allproducts[aproduct['asin']] = aproduct\n",
    "    \n",
    "    for categories in aproduct['categories']:\n",
    "        for acategory in categories:\n",
    "            if acategory in listofcategories:\n",
    "                listofcategories[acategory] += 1\n",
    "            if acategory not in listofcategories:\n",
    "                listofcategories[acategory] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06651660520532346\n",
      "0.13303321041064692\n",
      "0.19954981561597038\n",
      "0.26606642082129384\n",
      "0.33258302602661727\n",
      "0.39909963123194075\n",
      "0.4656162364372642\n",
      "0.5321328416425877\n",
      "0.5986494468479111\n",
      "0.6651660520532345\n",
      "0.7316826572585581\n",
      "0.7981992624638815\n",
      "0.8647158676692049\n",
      "0.9312324728745284\n",
      "0.9977490780798518\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "allspeedoasins = set()\n",
    "\n",
    "for aproduct in allproducts:\n",
    "     theproduct = allproducts[aproduct]\n",
    "     count += 1\n",
    "     if count % 100000 == 0:\n",
    "         print(count/1503384)\n",
    "     for categories in theproduct['categories']:\n",
    "        for acategory in categories:\n",
    "            if 'speedo' in acategory.lower():\n",
    "                allspeedoasins.add(theproduct['asin'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all products in the data is: \n",
      "1503384\n"
     ]
    }
   ],
   "source": [
    "print (\"length of all products in the data is: \")\n",
    "print(len(allproducts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of Speedo products is: \n",
      "1625\n"
     ]
    }
   ],
   "source": [
    "print (\"length of Speedo products is: \")\n",
    "print(len(allspeedoasins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = open('allasins.txt', 'w')\n",
    "# Opening a text file \n",
    "\n",
    "outputfile.write(','.join(allspeedoasins))\n",
    "#overwriting the text file by joining allspeedoasins \n",
    "\n",
    "outputfile.close()\n",
    "# Closing the file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "    Secondly, I created a dictionary of all Speedo product reviews then extracted the reviews from review data and    dumped them into a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedjson = open('reviews_Clothing_Shoes_and_Jewelry.json','r')\n",
    "# loading json data of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n"
     ]
    }
   ],
   "source": [
    "allreviews = {}\n",
    "count = 0\n",
    "\n",
    "for aline in loadedjson:\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print(count)\n",
    "    areview = eval(aline)\n",
    "    \n",
    "    theasin = areview['asin']\n",
    "    thereviewer = areview['reviewerID']\n",
    "    \n",
    "    if theasin in allspeedoasins:\n",
    "        thekey = '%s,%s'%(theasin,thereviewer)\n",
    "        allreviews[thekey] = areview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of reviews of Speedo is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9573"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Length of reviews of Speedo is: \")\n",
    "len(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "        \n",
    "json.dump(allreviews,open('allspeedoreviews.json','w'))  # allspeedoreviews\n",
    "\n",
    "allreviews = json.load(open('allspeedoreviews.json','r'))\n",
    "# This code will open the json file created above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "    In the next step I have removed the stop-words like “a”, “an” as these words are so common  in English sentence that they can be over represented in the results. For this, I have imported nltk(natural language toolkit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adititripathi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk    \n",
    "# removing stopwords from the reviews here to fetch better topic terms\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.append('speedo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "    Here I have defined a function that would load the asins, summary and reviewtext together in a set for further    processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load_texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            print(topicdata[areview].keys())\n",
    "            reviewtext = topicdata[areview]['reviewText']\n",
    "            summary = topicdata[areview]['summary']\n",
    "            asin = topicdata[areview]['asin']\n",
    "            review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "            \n",
    "            texts.add(review)\n",
    "\n",
    "#load_texts(allreviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model I chose for this project is K-means clustering.\n",
    "\n",
    "    At first I have imported TfidVectorizer from sklearn feature extraction and K means from sklearn cluster.\n",
    "    K – means initializes with a pre-determined number of clusters which is 22 in my model.\n",
    "    The maximum iteration which I set in my model is 100000 i.e my model would only iterate the process until \n",
    "    100000 times.\n",
    "    The next step involves creating an organized list of topics.\n",
    "    After having the topics, I have create a directory for classification of topic terms thereby extracting the \n",
    "    topic terms from each topic.\n",
    "    A dictionary entry is created for each topic which is initially empty. The topic terms are used to name files  and at the end of the loop the file is closed to make sure that nothing could be read & written further in it.\n",
    "\n",
    "\n",
    "# Model 1\n",
    "\n",
    "    Basic Model with no additional conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: stars five four three\n",
      "1: pool shoes feet great\n",
      "2: tight size like top\n",
      "3: watch band lap time\n",
      "4: good quality fit great\n",
      "5: small size runs suit\n",
      "6: trunks swim great fit\n",
      "7: bag b000791api gear equipment\n",
      "8: nice fit color good\n",
      "9: shirt b005qxesww sun great\n",
      "10: size ordered big smaller\n",
      "11: great fit one well\n",
      "12: shorts great swim board\n",
      "13: 34 size fit 32\n",
      "14: comfortable great shoes beach\n",
      "15: old daughter year loves\n",
      "16: shoe water great shoes\n",
      "17: swimsuit great fit daughter\n",
      "18: shoes water great b0010odjnk\n",
      "19: product great good quality\n",
      "20: suit great bathing swim\n",
      "21: flip flops flop comfortable\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   \n",
    "# Assigning number of topics, it can be changed later analysing the overlaps among topics.  \n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    # it extracts the topic terms from each topic term\n",
    "    outfiles[atopic] = open(os.path.join('output', '_'.join(topic_terms) + '.txt'), 'w')  \n",
    "    #creating a dictionary entry for a topic, and I am taking those \n",
    "    #topic terms and using them to name the files\n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        #predictions = model.predict(Y)\n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            #here I am making sure by piping in,that I am adding a line break\n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    1: trunks swim fit great - Wide range of Men's shorts are comfortable, fit well & holds color.\n",
    "    3: aerobics water class shoes -  Water shoes by Speedo which are perfect for water aerobics.\n",
    "    5: shirt b005qxesww sun great - Speedo rash guard t-shirt provides great fit, sun protection & comes in \n",
    "    good quality nylon fabric.\n",
    "    19: watch band lap time - Speedo Aquacoach watch has short strap opposed to size mentioned on site. It measures every stroke distance & time accurately.\n",
    "    21: bag b000791api equipment great -  The mesh bag is big enough to store plenty of stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "\n",
    "    Negative Reviews - starred less than 2 in 2013   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load2texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            if 'overall' in topicdata[areview]:\n",
    "                if int(topicdata[areview]['overall']) < 2:\n",
    "                    reviewdate = topicdata[areview]['reviewTime']\n",
    "                    datenocomma = reviewdate.replace(\",\", \"\")\n",
    "                    mmddyy = datenocomma.split(' ')\n",
    "                    if mmddyy[2] == \"2013\":\n",
    "                        summary = topicdata[areview]['summary']\n",
    "                        reviewtext = topicdata[areview]['reviewText']\n",
    "                        asin = topicdata[areview]['asin']\n",
    "                        review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "                        texts.add(review)\n",
    "\n",
    "#load2texts(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: wet pool day got\n",
      "1: shoe water shoes return\n",
      "2: came stitching time first\n",
      "3: suit b0096s1592 modest back\n",
      "4: use months started item\n",
      "5: apart fell months shoes\n",
      "6: watch time broke counter\n",
      "7: flip plastic flops price\n",
      "8: material fit size difficult\n",
      "9: shoes feet water never\n",
      "10: small size way 34\n",
      "11: tight look picture like\n",
      "12: sizing size shirt small\n",
      "13: shorts told vietnam pockets\n",
      "14: faded fabric color one\n",
      "15: wet shoes well slipped\n",
      "16: suit poor quality fabric\n",
      "17: arch makes uncomfortable something\n",
      "18: thin toe unglued comes\n",
      "19: quality bad good buy\n",
      "20: heel money throw hated\n",
      "21: seller product warranty worn\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   # Assigning number of topics\n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Classification of topic terms\n",
    "\n",
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output2')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    outfiles[atopic] = open(os.path.join('output2', '_'.join(topic_terms) + '.txt'), 'w')  \n",
    "    \n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "       \n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "           \n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    2: wet beware dangerous hurt - Speedo shoes are quite comfortable but slip on wet floors.\n",
    "    12: sizing chart misleading small - Fabric is stretchy and comfortable, but sizing is off!\n",
    "    14: narrow brand inch side -  The side edges of brief are too narrow: highly disappointing.\n",
    "    15: watch buy counter return -  Watch is good but band is ugly.\n",
    "    21: flip flops toes plastic - The strap is made up of horrible plastic and cause blisters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "\n",
    "    Positive Reviews - starred more than 4 in 2013  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load3texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            if 'overall' in topicdata[areview]:\n",
    "                if int(topicdata[areview]['overall']) > 4:\n",
    "                    reviewdate = topicdata[areview]['reviewTime']\n",
    "                    datenocomma = reviewdate.replace(\",\", \"\")\n",
    "                    mmddyy = datenocomma.split(' ')\n",
    "                    if mmddyy[2] == \"2013\":\n",
    "                        summary = topicdata[areview]['summary']\n",
    "                        reviewtext = topicdata[areview]['reviewText']\n",
    "                        asin = topicdata[areview]['asin']\n",
    "                        review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "                        texts.add(review)\n",
    "\n",
    "load3texts(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: beach great perfect comfortable\n",
      "1: good quality price nice\n",
      "2: suit great swim fits\n",
      "3: shoes water great b0010odjnk\n",
      "4: swimsuit like suits swimming\n",
      "5: runs size small large\n",
      "6: love great comfortable fit\n",
      "7: b001ibiwqy comfortable great sandal\n",
      "8: trunks swim fit great\n",
      "9: shoe water great comfortable\n",
      "10: suit bathing fits daughter\n",
      "11: one big size fit\n",
      "12: shorts swim great board\n",
      "13: 34 fit great fits\n",
      "14: size order one larger\n",
      "15: fit perfect size ordered\n",
      "16: pool aerobics water great\n",
      "17: old year loves daughter\n",
      "18: bag b000791api gear swim\n",
      "19: product great expected fit\n",
      "20: great water shoes comfortable\n",
      "21: shirt sun protection b005qxet1m\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   # Assigning number of topics\n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Classification of topic terms\n",
    "\n",
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output3')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    outfiles[atopic] = open(os.path.join('output3', '_'.join(topic_terms) + '.txt'), 'w') \n",
    "    \n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "           \n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    0: brand chose swimmer perfect -   Recommended for everyday swimmer.\n",
    "    1: great price value fit -  SPEEDO WOMEN'S SURFWALKER PRO - A great product. Costed a few more dollars, but \n",
    "    well worth the price.\n",
    "    4: quality nice good little -  Speedo Men's Solid Jammer The material is great and very sleek design.\n",
    "    7: shoes water great comfortable -  GREAT FOR NON-SLIP! These shoes are perfect for cave tubing.\n",
    "    13: old year loves daughter -   Baby girl products are adorable and secure.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4\n",
    "\n",
    "    Negative Reviews - starred less than 2 in 2014 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load4texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            if 'overall' in topicdata[areview]:\n",
    "                if int(topicdata[areview]['overall']) < 2:\n",
    "                    reviewdate = topicdata[areview]['reviewTime']\n",
    "                    datenocomma = reviewdate.replace(\",\", \"\")\n",
    "                    mmddyy = datenocomma.split(' ')\n",
    "                    if mmddyy[2] == \"2014\":\n",
    "                        summary = topicdata[areview]['summary']\n",
    "                        reviewtext = topicdata[areview]['reviewText']\n",
    "                        asin = topicdata[areview]['asin']\n",
    "                        review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "                        texts.add(review)\n",
    "\n",
    "load4texts(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: worked lap minutes got\n",
      "1: small waist fit like\n",
      "2: shoes return small water\n",
      "3: shoe give disappointed size\n",
      "4: short shorts b0047e59lu 8217\n",
      "5: apart strap b001ibiwqy foot\n",
      "6: size small ordered large\n",
      "7: stopped working received days\n",
      "8: suit lining bottom compression\n",
      "9: watch water horrible died\n",
      "10: flops flip 34 feet\n",
      "11: buttons even instructions directions\n",
      "12: see good soon b004477t18\n",
      "13: quality poor pair years\n",
      "14: big color back send\n",
      "15: inside seams never blisters\n",
      "16: 34 black wear ya\n",
      "17: waterproof one news recomend\n",
      "18: water well time first\n",
      "19: bag hard making complaint\n",
      "20: skin b006pkj13g small son\n",
      "21: broken sent replacement use\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   # Assigning number of topics\n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output4')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    outfiles[atopic] = open(os.path.join('output4', '_'.join(topic_terms) + '.txt'), 'w')  \n",
    "    \n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            \n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()     \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    2: pair apart poor quality - Non-Standard sizes and dense tight fabric – Speedo jammers\n",
    "    7: watch water horrible using - Speedo watches have technical issues, stops working in few days of use.\n",
    "    20: tight strange fit boring - Inappropriate range of size – XL is not a regular XL.\n",
    "    21: strap apart b001ibiwqy came -  Disaster Strap! Men’s pool slide.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5\n",
    "\n",
    "    Positive Reviews - starred more than 4 in 2014 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load5texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            if 'overall' in topicdata[areview]:\n",
    "                if int(topicdata[areview]['overall']) > 4:\n",
    "                    reviewdate = topicdata[areview]['reviewTime']\n",
    "                    datenocomma = reviewdate.replace(\",\", \"\")\n",
    "                    mmddyy = datenocomma.split(' ')\n",
    "                    if mmddyy[2] == \"2014\":\n",
    "                        summary = topicdata[areview]['summary']\n",
    "                        reviewtext = topicdata[areview]['reviewText']\n",
    "                        asin = topicdata[areview]['asin']\n",
    "                        review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "                        texts.add(review)\n",
    "\n",
    "load5texts(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: jammers b0006uzx9m fit going\n",
      "1: 34 fit great perfect\n",
      "2: pool b000c3vb1m great foot\n",
      "3: suit great bathing fits\n",
      "4: tight little like size\n",
      "5: easy water comfortable b0010odjnk\n",
      "6: love fit perfectly b0010odjnk\n",
      "7: comfortable great fit perfect\n",
      "8: well made fit great\n",
      "9: five stars great good\n",
      "10: loves daughter suit swim\n",
      "11: shoe water great fit\n",
      "12: size small great 10\n",
      "13: long love sun shorts\n",
      "14: shoes water b0010odjnk great\n",
      "15: shorts swim great fit\n",
      "16: bag b000791api gear great\n",
      "17: shirt b005qxesww great nice\n",
      "18: old year suit daughter\n",
      "19: one size larger suit\n",
      "20: good quality product great\n",
      "21: cute fit great wear\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   # Assigning number of topics\n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output5')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    outfiles[atopic] = open(os.path.join('output5', '_'.join(topic_terms) + '.txt'), 'w')  \n",
    "    \n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            \n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    3: good quality fit well - Great result: for wide variety of footwear.\n",
    "    13: excellent beach walk rocks - Great for comfortable beachcoming: Speedo Surfwalkers\n",
    "    14: suit great bathing fits -  A wide variety of Men’s swimsuit tend to fit comfortably.\n",
    "    21: looking well great made -  Brief swimsuit: snug fitting.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6\n",
    "\n",
    "    Reviews during summer 2013(June - August)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load6texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            reviewdate = topicdata[areview]['reviewTime']\n",
    "            datenocomma = reviewdate.replace(\",\", \"\")\n",
    "            mmddyy = datenocomma.split(' ')\n",
    "            if (int(mmddyy[0])>5 and int(mmddyy[0])<9):\n",
    "                summary = topicdata[areview]['summary']\n",
    "                reviewtext = topicdata[areview]['reviewText']\n",
    "                asin = topicdata[areview]['asin']\n",
    "                review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "                texts.add(review)\n",
    "\n",
    "load6texts(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: suit great bathing fits\n",
      "1: top one great like\n",
      "2: swimsuit great daughter fits\n",
      "3: good fit well fits\n",
      "4: size small ordered tight\n",
      "5: watch band lap laps\n",
      "6: beach sand shoes great\n",
      "7: pool pair comfortable great\n",
      "8: shoes feet water pair\n",
      "9: stars four three two\n",
      "10: shirt great b005qxesww sun\n",
      "11: five stars great good\n",
      "12: large size big medium\n",
      "13: perfect fit daughter loves\n",
      "14: shoes water great b0010odjnk\n",
      "15: quality good size nice\n",
      "16: shoe water size great\n",
      "17: b002mukiz8 comfortable wear water\n",
      "18: product great good quality\n",
      "19: bag b000791api gear mesh\n",
      "20: shorts swim great short\n",
      "21: 34 size fit waist\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   # Assigning number of topics\n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output6')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    outfiles[atopic] = open(os.path.join('output6', '_'.join(topic_terms) + '.txt'), 'w')  \n",
    "    \n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            \n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    3: 34 suit fit waist - Great jammer! Snug where It's supposed to be, comfortable where you want it to be.\n",
    "    14: bathing suit suits great -  Toddler bathing suit: comfortable and very good in quality.\n",
    "    17: shirt great b005qxesww sun -  Comfortable and light weight beach shirt.\n",
    "    21: shoe water great good -  Speedo Women's Surfwalker Pro: Great Purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 7\n",
    "\n",
    "    Reviews during summer 2014(June - August)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set()\n",
    "\n",
    "def load7texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            reviewdate = topicdata[areview]['reviewTime']\n",
    "            datenocomma = reviewdate.replace(\",\", \"\")\n",
    "            mmddyy = datenocomma.split(' ')\n",
    "            if (int(mmddyy[0])>5 and int(mmddyy[0])<9):\n",
    "                summary = topicdata[areview]['summary']\n",
    "                reviewtext = topicdata[areview]['reviewText']\n",
    "                asin = topicdata[areview]['asin']\n",
    "                review = '%s %s %s' %(asin,summary,reviewtext)\n",
    "                texts.add(review)\n",
    "\n",
    "load7texts(allreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: bag b000791api gear mesh\n",
      "1: beach comfortable pool great\n",
      "2: small size runs suit\n",
      "3: cute love great b000c3vb1m\n",
      "4: four stars b00eymys5s shirt\n",
      "5: top suit size bottom\n",
      "6: size big ordered large\n",
      "7: stars five three great\n",
      "8: shirt b005qxesww great sun\n",
      "9: perfect fit great quality\n",
      "10: great fit product comfortable\n",
      "11: 34 fit waist size\n",
      "12: nice well fits swimsuit\n",
      "13: one would fit time\n",
      "14: old year loves daughter\n",
      "15: good quality fit product\n",
      "16: shoe water great shoes\n",
      "17: shoes water great fit\n",
      "18: shorts swim great short\n",
      "19: feet shoes water pool\n",
      "20: watch band lap laps\n",
      "21: suit great bathing fits\n"
     ]
    }
   ],
   "source": [
    "documents = list(texts)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 22   # Assigning number of topics\n",
    "\n",
    "model = KMeans(n_clusters = true_k,\n",
    "               max_iter = 100000)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster\")\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' %(i, ' '. join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "outfiles = {}\n",
    "\n",
    "try:\n",
    "    os.mkdir('output7')\n",
    "    \n",
    "except OSError:  \n",
    "    print(\"directory already exists\")\n",
    "\n",
    "else:\n",
    "    print (\"Successfully created the directory\")\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[atopic, :4]] \n",
    "    outfiles[atopic] = open(os.path.join('output7', '_'.join(topic_terms) + '.txt'), 'w')  \n",
    "    \n",
    "\n",
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        reviewwithmetadata = \"%s %s %s\" % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        \n",
    "        \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            \n",
    "\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()             \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most relevant topic terms and their descriptions.\n",
    "\n",
    "    1: watch band lap laps -  Watch is waterproof but has ugly band and uneasy to wear.\n",
    "    6: shirt great sun b005qxesww - Shirt which provides awesome sun protection at the beach.\n",
    "    17: shoes water great comfortable -  Speedo Women's Surfwalker Pro: Quality water shoes.\n",
    "    21: 34 suit fit size - Size variations: does not follow the standard range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
